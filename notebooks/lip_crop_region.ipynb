{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd685a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import qai_hub as hub\n",
    "from qai_hub_models.models.mediapipe_face import Model\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "from helper import random_color_images_batch, process_image_for_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec69cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = random_color_images_batch(batch_size=8, csv_path=\"file_paths.csv\")\n",
    "images_processed = process_image_for_model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bae36491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[160, 189, 216],\n",
       "        [189, 213, 235],\n",
       "        [203, 221, 232],\n",
       "        ...,\n",
       "        [162, 159, 175],\n",
       "        [168, 160, 177],\n",
       "        [165, 157, 174]],\n",
       "\n",
       "       [[171, 201, 226],\n",
       "        [198, 223, 243],\n",
       "        [207, 226, 234],\n",
       "        ...,\n",
       "        [161, 158, 174],\n",
       "        [165, 159, 176],\n",
       "        [165, 157, 174]],\n",
       "\n",
       "       [[161, 192, 215],\n",
       "        [193, 219, 236],\n",
       "        [206, 225, 232],\n",
       "        ...,\n",
       "        [160, 157, 173],\n",
       "        [164, 158, 175],\n",
       "        [163, 157, 174]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[126, 138, 138],\n",
       "        [123, 135, 135],\n",
       "        [124, 136, 136],\n",
       "        ...,\n",
       "        [133, 137, 155],\n",
       "        [131, 137, 150],\n",
       "        [128, 134, 145]],\n",
       "\n",
       "       [[124, 135, 133],\n",
       "        [123, 134, 132],\n",
       "        [125, 136, 134],\n",
       "        ...,\n",
       "        [137, 136, 156],\n",
       "        [135, 135, 151],\n",
       "        [130, 133, 147]],\n",
       "\n",
       "       [[126, 137, 135],\n",
       "        [126, 137, 135],\n",
       "        [129, 137, 136],\n",
       "        ...,\n",
       "        [138, 136, 156],\n",
       "        [134, 134, 150],\n",
       "        [131, 132, 146]]], dtype=uint8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f151a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.is_tensor(images):\n",
    "    images = images.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "457db196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[160, 189, 216],\n",
       "        [189, 213, 235],\n",
       "        [203, 221, 232],\n",
       "        ...,\n",
       "        [162, 159, 175],\n",
       "        [168, 160, 177],\n",
       "        [165, 157, 174]],\n",
       "\n",
       "       [[171, 201, 226],\n",
       "        [198, 223, 243],\n",
       "        [207, 226, 234],\n",
       "        ...,\n",
       "        [161, 158, 174],\n",
       "        [165, 159, 176],\n",
       "        [165, 157, 174]],\n",
       "\n",
       "       [[161, 192, 215],\n",
       "        [193, 219, 236],\n",
       "        [206, 225, 232],\n",
       "        ...,\n",
       "        [160, 157, 173],\n",
       "        [164, 158, 175],\n",
       "        [163, 157, 174]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[126, 138, 138],\n",
       "        [123, 135, 135],\n",
       "        [124, 136, 136],\n",
       "        ...,\n",
       "        [133, 137, 155],\n",
       "        [131, 137, 150],\n",
       "        [128, 134, 145]],\n",
       "\n",
       "       [[124, 135, 133],\n",
       "        [123, 134, 132],\n",
       "        [125, 136, 134],\n",
       "        ...,\n",
       "        [137, 136, 156],\n",
       "        [135, 135, 151],\n",
       "        [130, 133, 147]],\n",
       "\n",
       "       [[126, 137, 135],\n",
       "        [126, 137, 135],\n",
       "        [129, 137, 136],\n",
       "        ...,\n",
       "        [138, 136, 156],\n",
       "        [134, 134, 150],\n",
       "        [131, 132, 146]]], dtype=uint8)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4fc358c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 192, 192])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.is_tensor(images_processed)\n",
    "\n",
    "images_processed[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "702f9328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.0001, 0.0003, 0.0001, 0.0108, 0.0018, 0.0002, 0.0006, 0.0002]), tensor([[[ 0.4501,  0.6709, -0.0351],\n",
      "         [ 0.4645,  0.6005, -0.1170],\n",
      "         [ 0.4547,  0.6193, -0.0541],\n",
      "         ...,\n",
      "         [ 0.4951,  0.4388, -0.0321],\n",
      "         [ 0.6231,  0.4167,  0.0254],\n",
      "         [ 0.6363,  0.4010,  0.0269]],\n",
      "\n",
      "        [[ 0.4701,  0.6759, -0.0403],\n",
      "         [ 0.4574,  0.6011, -0.1113],\n",
      "         [ 0.4688,  0.6228, -0.0541],\n",
      "         ...,\n",
      "         [ 0.5027,  0.4469, -0.0330],\n",
      "         [ 0.6409,  0.4209, -0.0102],\n",
      "         [ 0.6541,  0.4040, -0.0116]],\n",
      "\n",
      "        [[ 0.4997,  0.7153, -0.0264],\n",
      "         [ 0.4716,  0.6449, -0.1087],\n",
      "         [ 0.4943,  0.6603, -0.0461],\n",
      "         ...,\n",
      "         [ 0.5298,  0.4717, -0.0565],\n",
      "         [ 0.6804,  0.4360, -0.0545],\n",
      "         [ 0.6948,  0.4164, -0.0585]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.5252,  0.6642, -0.0279],\n",
      "         [ 0.5331,  0.5944, -0.1004],\n",
      "         [ 0.5326,  0.6119, -0.0444],\n",
      "         ...,\n",
      "         [ 0.5871,  0.4384, -0.0395],\n",
      "         [ 0.7141,  0.4165, -0.0101],\n",
      "         [ 0.7288,  0.3982, -0.0108]],\n",
      "\n",
      "        [[ 0.4695,  0.6945, -0.0348],\n",
      "         [ 0.4732,  0.6202, -0.1209],\n",
      "         [ 0.4717,  0.6387, -0.0549],\n",
      "         ...,\n",
      "         [ 0.5176,  0.4473, -0.0406],\n",
      "         [ 0.6591,  0.4223,  0.0038],\n",
      "         [ 0.6733,  0.4044,  0.0038]],\n",
      "\n",
      "        [[ 0.4825,  0.6958, -0.0292],\n",
      "         [ 0.4905,  0.6260, -0.1112],\n",
      "         [ 0.4863,  0.6436, -0.0482],\n",
      "         ...,\n",
      "         [ 0.5266,  0.4599, -0.0387],\n",
      "         [ 0.6572,  0.4340,  0.0078],\n",
      "         [ 0.6700,  0.4183,  0.0082]]]))\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "torch_model = Model.from_pretrained()\n",
    "\n",
    "# Device\n",
    "device = hub.Device(\"CPU\")\n",
    "results = torch_model.face_landmark_detector(images_processed)\n",
    "\n",
    "# print(dir(torch_model))\n",
    "print(results)\n",
    "# Trace Model\n",
    "# input_shape = torch_model.get_input_spec()\n",
    "# sample_inputs = torch_model.sample_inputs()\n",
    "\n",
    "# print(f\"Input shape: {input_shape} \\n Sample Inputs: {sample_inputs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ad32040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def draw_landmarks(img, points, connections=[], color=(0, 255, 0), size=2):\n",
    "    h, w = img.shape[:2]\n",
    "    points = points.copy()\n",
    "    for point in points:\n",
    "        x, y = point[:2]\n",
    "        if 0 <= x <= 1 and 0 <= y <= 1:\n",
    "            x = int(x * w)\n",
    "            y = int(y * h)\n",
    "            cv2.circle(img, (x, y), size, color, thickness=size)\n",
    "    # Draw connections if needed\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d008853",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = results[1][0]\n",
    "img    = images[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "01002a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 640, 3)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ab12080f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.is_tensor(points):\n",
    "    points = points.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da251b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = draw_landmarks(img, points)\n",
    "cv2.imwrite(\"facial_detection.png\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lipreader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
