{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd685a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import qai_hub as hub\n",
    "from qai_hub_models.models.mediapipe_face import Model\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "from helper import random_color_images_batch, process_image_for_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b9c63e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pradh\\miniconda3\\envs\\lipreader\\lib\\site-packages\\qai_hub_models\\utils\\quantization_aimet_onnx.py:33: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from qai_hub_models.models.mediapipe_face import demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec69cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = random_color_images_batch(batch_size=8, csv_path=\"file_paths.csv\")\n",
    "images_processed = process_image_for_model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bae36491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[169, 193, 199],\n",
       "        [167, 193, 200],\n",
       "        [167, 194, 204],\n",
       "        ...,\n",
       "        [151, 147, 159],\n",
       "        [141, 144, 158],\n",
       "        [146, 154, 167]],\n",
       "\n",
       "       [[166, 192, 198],\n",
       "        [165, 193, 200],\n",
       "        [166, 195, 204],\n",
       "        ...,\n",
       "        [158, 154, 166],\n",
       "        [151, 152, 166],\n",
       "        [151, 157, 170]],\n",
       "\n",
       "       [[163, 195, 200],\n",
       "        [161, 195, 201],\n",
       "        [162, 198, 206],\n",
       "        ...,\n",
       "        [165, 163, 175],\n",
       "        [162, 162, 174],\n",
       "        [160, 162, 173]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 39,  49,  36],\n",
       "        [ 37,  50,  34],\n",
       "        [ 33,  49,  31],\n",
       "        ...,\n",
       "        [138, 138, 150],\n",
       "        [136, 136, 148],\n",
       "        [132, 132, 144]],\n",
       "\n",
       "       [[ 42,  46,  27],\n",
       "        [ 37,  43,  24],\n",
       "        [ 33,  45,  23],\n",
       "        ...,\n",
       "        [139, 137, 149],\n",
       "        [136, 135, 145],\n",
       "        [132, 131, 141]],\n",
       "\n",
       "       [[ 41,  42,  22],\n",
       "        [ 35,  38,  16],\n",
       "        [ 34,  42,  19],\n",
       "        ...,\n",
       "        [139, 137, 149],\n",
       "        [136, 132, 143],\n",
       "        [133, 129, 140]]], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f151a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.is_tensor(images):\n",
    "    images = images.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "457db196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[169, 193, 199],\n",
       "        [167, 193, 200],\n",
       "        [167, 194, 204],\n",
       "        ...,\n",
       "        [151, 147, 159],\n",
       "        [141, 144, 158],\n",
       "        [146, 154, 167]],\n",
       "\n",
       "       [[166, 192, 198],\n",
       "        [165, 193, 200],\n",
       "        [166, 195, 204],\n",
       "        ...,\n",
       "        [158, 154, 166],\n",
       "        [151, 152, 166],\n",
       "        [151, 157, 170]],\n",
       "\n",
       "       [[163, 195, 200],\n",
       "        [161, 195, 201],\n",
       "        [162, 198, 206],\n",
       "        ...,\n",
       "        [165, 163, 175],\n",
       "        [162, 162, 174],\n",
       "        [160, 162, 173]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 39,  49,  36],\n",
       "        [ 37,  50,  34],\n",
       "        [ 33,  49,  31],\n",
       "        ...,\n",
       "        [138, 138, 150],\n",
       "        [136, 136, 148],\n",
       "        [132, 132, 144]],\n",
       "\n",
       "       [[ 42,  46,  27],\n",
       "        [ 37,  43,  24],\n",
       "        [ 33,  45,  23],\n",
       "        ...,\n",
       "        [139, 137, 149],\n",
       "        [136, 135, 145],\n",
       "        [132, 131, 141]],\n",
       "\n",
       "       [[ 41,  42,  22],\n",
       "        [ 35,  38,  16],\n",
       "        [ 34,  42,  19],\n",
       "        ...,\n",
       "        [139, 137, 149],\n",
       "        [136, 132, 143],\n",
       "        [133, 129, 140]]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4fc358c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 192, 192])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.is_tensor(images_processed)\n",
    "\n",
    "images_processed[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "702f9328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from qai_hub_models.models.mediapipe_face.app import MediaPipeFaceApp\n",
    "from qai_hub_models.utils.asset_loaders import load_image\n",
    "from qai_hub_models.utils.display import display_or_save_image\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "torch_model = Model.from_pretrained()\n",
    "\n",
    "# Device\n",
    "device = hub.Device(\"CPU\")\n",
    "landmark_detector = torch_model.face_landmark_detector\n",
    "detector    = torch_model.face_detector\n",
    "\n",
    "\n",
    "app = MediaPipeFaceApp(\n",
    "        detector,  # type: ignore\n",
    "        landmark_detector,  # type: ignore\n",
    "        torch_model.face_detector.anchors,\n",
    "        torch_model.face_detector.get_input_spec(),\n",
    "        torch_model.face_landmark_detector.get_input_spec(),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6abc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving image to ./test.png\\image.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pradh\\AppData\\Local\\Temp\\ipykernel_30480\\193779454.py:7: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  out_image = Image.fromarray(pred_image[0], \"RGB\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = '01'\n",
    "images_path = f\"..\\data_raw\\F01\\phrases\\{num}\\{num}\\color_001.jpg\"\n",
    "image = load_image(images_path).convert(\"RGB\")\n",
    "pred_image = app.predict_landmarks_from_image(image)\n",
    "\n",
    "assert isinstance(pred_image[0], np.ndarray)\n",
    "out_image = Image.fromarray(pred_image[0], \"RGB\")\n",
    "\n",
    "display_or_save_image(out_image, './test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lipreader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
